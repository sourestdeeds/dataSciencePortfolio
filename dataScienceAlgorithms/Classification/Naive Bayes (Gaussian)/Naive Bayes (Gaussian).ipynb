{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Gaussian)\n",
    "\n",
    "A typical example of problem ML tries to solve is classification. It can be expressed as the ability, given some input data, to assign a ‘class label’ to a sample.\n",
    "\n",
    "To make things clearer, let’s make an example. Imagine we performed analysis on samples of objects and we collected their specs. Now, given this information, we would like to know if that object is a window glass (from vehicle or building) or not a window glass (containers, tableware, or headlamps). Unfortunately, we do not have a formula which, given these values, will provide us with the answer.\n",
    "\n",
    "Someone who has handled glasses might be able to tell just by looking or touching it if that is a window glass or not. That is because he has acquired experience by looking at many examples of different kind of glasses. That is exactly what happens with machine learning. We say that we ‘train’ the algorithm to learn from known examples.\n",
    "\n",
    "We provide a ‘training set’ where we specify both the input specs of the class and its category. The algorithm goes through the examples, learns the distinctive features of a window glass and so it can infer the class of a given uncategorized example.\n",
    "\n",
    "We will use a dataset titled ‘Glass Identification Database’, created by B. German from Central Research Establishment Home Office Forensic Science Service. The original dataset classified the glass into 7 classes: 4 types of window glass classes, and 3 types of non-window glass classes. Our version treats all 4 types of window glass classes as one class, and all 3 types of non-window glass classes as one class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row is an example and contains 11 attributes as listed below.\n",
    "\n",
    "- Example number\n",
    "- RI: refractive index\n",
    "- Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n",
    "- Mg: Magnesium\n",
    "- Al: Aluminum\n",
    "- Si: Silicon\n",
    "- K: Potassium\n",
    "- Ca: Calcium\n",
    "- Ba: Barium\n",
    "- Fe: Iron\n",
    "- Type of glass: (class) – 1 window glass (from vehicle or building) – 2 not window glass (containers, tableware, or headlamps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "$ P(A|B) $ is the conditional probability of $A$, given $B$ (posterior probability), $P(B)$ is the prior probability of $B$ and $P(A)$ the prior probability of $A$. $P(B|A)$ is the conditional probability of $B$ given $A$, called the likely-hood.\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "ATTR_NAMES = [\"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\"]\n",
    "FIELD_NAMES = [\"Num\"] + ATTR_NAMES + [\"Class\"]\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', names=FIELD_NAMES)\n",
    "\n",
    "# Split x from y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[list_of_x_cols], df[y_col],test_size=0.2)\n",
    "# Split df\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "\n",
    "# Split df into train and test sets\n",
    "#train = data.sample(frac=0.8, random_state=200) # random state is a seed value\n",
    "#test = data.drop(train.index).sample(frac=1.0) # Shuffle the test set too\n",
    "\n",
    "X_train = train[[\"RI\", \"Na\"]].values\n",
    "Y_train = train['Class'].values\n",
    "\n",
    "X_test = test[[\"RI\", \"Na\"]].values\n",
    "Y_test = test['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_test, Y_test)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setted_list = [6+4*n for n in range(0,500)]\n",
    "value_chosen=65\n",
    "import bisect\n",
    "index = bisect.bisect(setted_list, value_chosen)\n",
    "\n",
    "setted_list[index-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

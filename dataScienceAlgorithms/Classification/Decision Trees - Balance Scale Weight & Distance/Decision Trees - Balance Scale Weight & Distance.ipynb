{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Balance Scale Weight & Distance Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions we make while using Decision tree :\n",
    "\n",
    "- At the beginning, we consider the whole training set as the root.\n",
    "- Attributes are assumed to be categorical for information gain and for gini index, attributes are assumed to be continuous.\n",
    "- On the basis of attribute values records are distributed recursively.\n",
    "- We use statistical methods for ordering attributes as root or internal node.\n",
    "\n",
    "Pseudocode :\n",
    "\n",
    "- Find the best attribute and place it on the root node of the tree.\n",
    "- Now, split the training set of the dataset into subsets. While making the subset make sure that each subset of training dataset should have the same value for an attribute.\n",
    "- Find leaf nodes in all branches by repeating 1 and 2 on each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare/Split data\n",
    "Data Slicing :\n",
    "- Before training the model we have to split the dataset into the training and testing dataset.\n",
    "- To split the dataset for training and testing we are using the sklearn module train_test_split\n",
    "- First of all we have to separate the target variable from the attributes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Index\n",
    "\n",
    "- Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.\n",
    "- It means an attribute with lower gini index should be preferred.\n",
    "- Sklearn supports “gini” criteria for Gini Index and by default, it takes “gini” value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "- Entropy is the measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. The higher the entropy the more the information content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "- The entropy typically changes when we use a node in a decision tree to partition the training instances into smaller subsets. Information gain is a measure of this change in entropy.\n",
    "- Sklearn supports “entropy” criteria for Information Gain and if we want to use Information Gain method in sklearn then we have to mention it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score\n",
    "\n",
    "- Accuracy score is used to calculate the accuracy of the trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "- Confusion Matrix is used to understand the trained classifier behavior over the test dataset or validate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:  625\n",
      "Dataset Shape:  (625, 5)\n",
      "Dataset:     0  1  2  3  4\n",
      "0  B  1  1  1  1\n",
      "1  R  1  1  1  2\n",
      "2  R  1  1  1  3\n",
      "3  R  1  1  1  4\n",
      "4  R  1  1  1  5\n",
      "Results Using Gini Index:\n",
      "Predicted values:\n",
      "['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L'\n",
      " 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L'\n",
      " 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R'\n",
      " 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n",
      " 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R'\n",
      " 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R']\n",
      "Confusion Matrix:  [[ 0  6  7]\n",
      " [ 0 67 18]\n",
      " [ 0 19 71]]\n",
      "Accuracy :  73.40425531914893\n",
      "Report :                precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.73      0.79      0.76        85\n",
      "           R       0.74      0.79      0.76        90\n",
      "\n",
      "    accuracy                           0.73       188\n",
      "   macro avg       0.49      0.53      0.51       188\n",
      "weighted avg       0.68      0.73      0.71       188\n",
      "\n",
      "Results Using Entropy:\n",
      "Predicted values:\n",
      "['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L'\n",
      " 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L'\n",
      " 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L'\n",
      " 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L'\n",
      " 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n",
      " 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R']\n",
      "Confusion Matrix:  [[ 0  6  7]\n",
      " [ 0 63 22]\n",
      " [ 0 20 70]]\n",
      "Accuracy :  70.74468085106383\n",
      "Report :                precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00        13\n",
      "           L       0.71      0.74      0.72        85\n",
      "           R       0.71      0.78      0.74        90\n",
      "\n",
      "    accuracy                           0.71       188\n",
      "   macro avg       0.47      0.51      0.49       188\n",
      "weighted avg       0.66      0.71      0.68       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Function importing Dataset\n",
    "def importdata():\n",
    "    balance_data = pd.read_csv(\n",
    "'https://archive.ics.uci.edu/ml/machine-learning-'+\n",
    "'databases/balance-scale/balance-scale.data',\n",
    "    sep= ',', header = None)\n",
    "      \n",
    "    # Printing the dataswet shape\n",
    "    print (\"Dataset Length: \", len(balance_data))\n",
    "    print (\"Dataset Shape: \", balance_data.shape)\n",
    "      \n",
    "    # Printing the dataset obseravtions\n",
    "    print (\"Dataset: \",balance_data.head())\n",
    "    return balance_data\n",
    "  \n",
    "# Function to split the dataset\n",
    "def splitdataset(balance_data):\n",
    "  \n",
    "    # Separating the target variable\n",
    "    X = balance_data.values[:, 1:5]\n",
    "    Y = balance_data.values[:, 0]\n",
    "  \n",
    "    # Splitting the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, Y, test_size = 0.3, random_state = 100)\n",
    "      \n",
    "    return X, Y, X_train, X_test, y_train, y_test\n",
    "      \n",
    "# Function to perform training with giniIndex.\n",
    "def train_using_gini(X_train, X_test, y_train):\n",
    "  \n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
    "            random_state = 100,max_depth=3, min_samples_leaf=5)\n",
    "  \n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "      \n",
    "# Function to perform training with entropy.\n",
    "def train_using_entropy(X_train, X_test, y_train):\n",
    "  \n",
    "    # Decision tree with entropy\n",
    "    clf_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 100,\n",
    "            max_depth = 3, min_samples_leaf = 5)\n",
    "  \n",
    "    # Performing training\n",
    "    clf_entropy.fit(X_train, y_train)\n",
    "    return clf_entropy\n",
    "  \n",
    "  \n",
    "# Function to make predictions\n",
    "def prediction(X_test, clf_object):\n",
    "  \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = clf_object.predict(X_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred\n",
    "      \n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test,y_pred)*100)\n",
    "      \n",
    "    print(\"Report : \",\n",
    "    classification_report(y_test, y_pred))\n",
    "  \n",
    "# Driver code\n",
    "def main():\n",
    "      \n",
    "    # Building Phase\n",
    "    data = importdata()\n",
    "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
    "    clf_gini = train_using_gini(X_train, X_test, y_train)\n",
    "    clf_entropy = train_using_entropy(X_train, X_test, y_train)\n",
    "      \n",
    "    # Operational Phase\n",
    "    print(\"Results Using Gini Index:\")\n",
    "      \n",
    "    # Prediction using gini\n",
    "    y_pred_gini = prediction(X_test, clf_gini)\n",
    "    cal_accuracy(y_test, y_pred_gini)\n",
    "      \n",
    "    print(\"Results Using Entropy:\")\n",
    "    # Prediction using entropy\n",
    "    y_pred_entropy = prediction(X_test, clf_entropy)\n",
    "    cal_accuracy(y_test, y_pred_entropy)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

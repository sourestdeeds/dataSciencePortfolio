{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Stump - Boosting\n",
    "The Boosting algorithm itself can strictly speaking neither learn nor predict anything since it is build kind of on top of some other (weak) algorithm. The Boosting algorithm is called a \"meta algorithm\". The Boosting approach can (as well as the bootstrapping approach), be applied, in principle, to any classification or regression algorithm but it turned out that tree models are especially suited. The accuracy of boosted trees turned out to be equivalent to Random Forests with respect and even often outperforms the latter (see for instance Caruana and Niculescu-Mizil (2008)(*An Empirical Comparison of Supervised Learning Algorithms*)). Hastie et al. (2009) call boosted decision trees the \"best off-the-shelf classifier of the world\" (Hastie et al. 2006 p.340). The mystic behind Boosting is in principal the same as for Random Forest models *-A bunch of weak learners which performs just slightly better than random guessing can be combined to make better predictions than one strong learner-*. Though, the process how these weak learners are created differs.\n",
    "Recapitulate, that during the creation of our Random Forest model we used the concept of Bagging. During Bagging we have grown a number of *M* trees where each was build on a random sample (allowing resampling) of the original dataset where the random sample had the same length as the original dataset but comprises only a randomly drawn subset of the total feature space. After we have created theses models, we let them make a majority vote to make our final decision. The quintessence is that each tree model is created independent from the outcomes of the other tree models. That is, the \"shape\" of the tree model is only influenced by the \"shape\" of the underlying data which in turn is only influenced by chance (*sampling with resampling*). The main difference in the creation of bagged trees using bootstrap aggregation and boosted trees using boosting is that we now replace the (random) resampling by some kind of *weighting* where we allocate the instances with weights and the weights of the ** tree depends on the results returned by the previously created () tree model. Hence, different from the Random Forest approach where we created an ensemble of tree models in parallel, we now create the ensemble in sequence, where the set up of the actual tree is influenced by the outputs of all the previous tree models by altering the weights of the dataset, the tree model is build on. The point is, that by implementing these weights, we introduce some kind of learning where the creation of the ** tree in the boosted model partly depends on the predictions the ** model has made. Therewith, we replace the more or less \"randomly-guided\" creation of the single datasets during bootstrapping by a \"guided\" creation. The most prominent boosting algorithm is called *AdaBoost* (adaptive boosting) and was developed by Freund and Schapire (1996). The following discussion is based on the AdaBoost Boosting algorithm. The following illustration gives a visual insight into the boosting algorithm.\n",
    "\n",
    "Lets first of all create a decision stump and measure the accuracy of this decision stump to get a feeling about the prediction \"goodness\" or rather \"badness\" of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all weights to $w = \\frac{1}{n}$ where  is the number of instances in the dataset\n",
    "\n",
    "- while $t < T$ (T==number of models to be grown) do:\n",
    "- Create a model and get the hypothesis $h_t(x_n)$ for all datapoints $x_n$ in the dataset\n",
    "- Calculate the error $\\epsilon$ of the training set summing over all datapoints $x_n$ in the training set where $I(cond)$ returns 1 if $I(cond)$ == True and 0 otherwise:\n",
    "\n",
    "$$ \\epsilon_t \\ \\ = \\ \\ \\ \\frac{\\sum_{n=1}^{N}w_n^{(t)}*I(y_n\\neq h_t(x_n))}{\\sum_{n=1}^{N}w_{n}^{(t)}} $$\n",
    "\n",
    "- Compute $\\alpha$ with:\n",
    "\n",
    "$$\\alpha_t\\ \\ =\\ \\ \\log(\\frac{1-\\epsilon_t}{\\epsilon_t})$$\n",
    "\n",
    "- Update the weights for the $N$ training instances in the next $(t+1)$ model with:\n",
    "\n",
    "$$w_{n}^{(t+1)}\\ \\ = \\ \\ w_{n}^{(t)}*exp(\\alpha_t*I(y_n\\neq h_t(x_n)))$$\n",
    "\n",
    "- After the $T$ iterations, calculate the final output with:\n",
    "\n",
    "$$f(x)\\ \\ = \\ \\ sign(\\sum_t^T \\alpha_t*h_t(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  73.06594399277326 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a Decision Stump\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "import scipy.stats as sps\n",
    "\n",
    "\n",
    "# Load in the data and define the column labels\n",
    "\n",
    "link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
    "dataset_base = pd.read_csv(link,header=None)\n",
    "dataset = pd.read_csv(link,header=None)\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.columns = ['target','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing',\n",
    "             'gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
    "             'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population',\n",
    "             'habitat']\n",
    "\n",
    "\n",
    "\n",
    "# Encode the feature values from strings to integers since the sklearn DecisionTreeClassifier only takes numerical values\n",
    "for label in dataset.columns:\n",
    "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
    "\n",
    "    \n",
    "    \n",
    "Tree_model = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
    "\n",
    "\n",
    "X = dataset.drop('target',axis=1)\n",
    "Y = dataset['target'].where(dataset['target']==1,-1)\n",
    "\n",
    "\n",
    "\n",
    "predictions = np.mean(cross_validate(Tree_model,X,Y,cv=100)['test_score'])\n",
    "\n",
    "\n",
    "print('The accuracy is: ',predictions*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that we have trained and tested the model on the same dataset (the whole dataset) using 100-fold Cross Validation. We get an accuracy of  73% which is not good but also not that terribly bad considering that we have used a decision stump for classification (split the dataset only once).\n",
    "\n",
    "Next, lets see how we can improve this result using a boosted decision stump approach. One thing which might be a bit confusing is that on our way to the final *boosted decision stump*, we use the whole dataset as training and testing dataset (we don't do a train test split). You might remember that we normally want to have a training set, on which we train the model and a testing set on which we test a model - Nevertheless, for Boosting we make an exception and use the whole dataset for training and testing - Just keep this exception in mind-."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a number of  50 base models we receive an accuracy of  98.67060561299851 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Boosting:\n",
    "\n",
    "    def __init__(self,dataset,T,test_dataset):\n",
    "        self.dataset = dataset\n",
    "        self.T = T\n",
    "        self.test_dataset = test_dataset\n",
    "        self.alphas = None\n",
    "        self.models = None\n",
    "        self.accuracy = []\n",
    "        self.predictions = None\n",
    "    \n",
    "    def fit(self):\n",
    "        # Set the descriptive features and the target feature\n",
    "        X = self.dataset.drop(['target'],axis=1)\n",
    "        Y = self.dataset['target'].where(self.dataset['target']==1,-1)\n",
    "\n",
    "        # Initialize the weights of each sample with wi = 1/N and create a dataframe in which the evaluation is computed\n",
    "        Evaluation = pd.DataFrame(Y.copy())\n",
    "        Evaluation['weights'] = 1/len(self.dataset) # Set the initial weights w = 1/N\n",
    "        \n",
    "\n",
    "        # Run the boosting algorithm by creating T \"weighted models\"\n",
    "        \n",
    "        alphas = [] \n",
    "        models = []\n",
    "        \n",
    "        for t in range(self.T):\n",
    "\n",
    "            # Train the Decision Stump(s)\n",
    "            Tree_model = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1) # Mind the deth one --> Decision Stump\n",
    "            \n",
    "            # We know that we must train our decision stumps on weighted datasets where the weights depend on the results of\n",
    "            # the previous decision stumps. To accomplish that, we use the 'weights' column of the above created \n",
    "            # 'evaluation dataframe' together with the sample_weight parameter of the fit method.\n",
    "            # The documentation for the sample_weights parameter sais: \"[...] If None, then samples are equally weighted.\"\n",
    "            # Consequently, if NOT None, then the samples are NOT equally weighted and therewith we create a WEIGHTED dataset \n",
    "            # which is exactly what we want to have.\n",
    "            model = Tree_model.fit(X,Y,sample_weight=np.array(Evaluation['weights'])) \n",
    "            \n",
    "            # Append the single weak classifiers to a list which is later on used to make the \n",
    "            # weighted decision\n",
    "            models.append(model)\n",
    "            predictions = model.predict(X)\n",
    "            score = model.score(X,Y)\n",
    "\n",
    "            # Add values to the Evaluation DataFrame\n",
    "            Evaluation['predictions'] = predictions\n",
    "            Evaluation['evaluation'] = np.where(Evaluation['predictions'] == Evaluation['target'],1,0)\n",
    "            Evaluation['misclassified'] = np.where(Evaluation['predictions'] != Evaluation['target'],1,0)\n",
    "\n",
    "            # Calculate the misclassification rate and accuracy\n",
    "            accuracy = sum(Evaluation['evaluation'])/len(Evaluation['evaluation'])\n",
    "            misclassification = sum(Evaluation['misclassified'])/len(Evaluation['misclassified'])\n",
    "\n",
    "\n",
    "            # Caclulate the error\n",
    "            err = np.sum(Evaluation['weights']*Evaluation['misclassified'])/np.sum(Evaluation['weights'])\n",
    " \n",
    "   \n",
    "            # Calculate the alpha values\n",
    "            alpha = np.log((1-err)/err)\n",
    "            alphas.append(alpha)\n",
    "\n",
    "\n",
    "            # Update the weights wi --> These updated weights are used in the sample_weight parameter\n",
    "            # for the training of the next decision stump. \n",
    "            Evaluation['weights'] *= np.exp(alpha*Evaluation['misclassified'])\n",
    "\n",
    "            #print('The Accuracy of the {0}. model is : '.format(t+1),accuracy*100,'%')\n",
    "            #print('The missclassification rate is: ',misclassification*100,'%')\n",
    "        \n",
    "        self.alphas = alphas\n",
    "        self.models = models\n",
    "            \n",
    "    def predict(self):\n",
    "        X_test = self.test_dataset.drop(['target'],axis=1).reindex(range(len(self.test_dataset)))\n",
    "        Y_test = self.test_dataset['target'].reindex(range(len(self.test_dataset))).where(self.dataset['target']==1,-1)\n",
    "    \n",
    "        # With each model in the self.model list, make a prediction \n",
    "        \n",
    "        accuracy = []\n",
    "        predictions = []\n",
    "        \n",
    "        for alpha,model in zip(self.alphas,self.models):\n",
    "            prediction = alpha*model.predict(X_test) # We use the predict method for the single decisiontreeclassifier models in the list\n",
    "            predictions.append(prediction)\n",
    "            self.accuracy.append(np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0]))\n",
    "            # The above line of code could be a little bit confusing and hence I will do up the single steps:\n",
    "            # Goal: Create a list of accuracies which can be used to plot the accuracy against the number of base learners used for the model\n",
    "            # 1. np.array(predictions) --> This is the array which contains the predictions of the single models. It has the shape 8124xn\n",
    "            # and hence looks like [[0.998,0.87,...0.87...],[...],[...],[0.99,1.23,...,1.05,0,99...]] \n",
    "            # 2. np.sum(np.array(predictions),axis=0) --> Summs up the first elements of the lists, that is 0,998+...+...+0.99. This is \n",
    "            # done since the formula for the prediction wants us to sum up the predictions of all models for each instance in the dataset. \n",
    "            # Hence if we have for example 3 models than the predictions array has the shape 8124x3 (Imagine a table with 3 columns and\n",
    "            # 8124 rows). Here the first column containst the predictions for the first model, the second column contains the \n",
    "            # prediction for the second model, the third column the prediction for the third model (mind that the\n",
    "            # second and third model are influenced by the results of the first resoectvely the first and the\n",
    "            # second model). This is logical since the results from column (model)\n",
    "            # n-1 are used to alter the weights of the nth model and the results of the nth model are then used to alter the weights\n",
    "            # of the n+1 model. \n",
    "            # 3. np.sign(np.sum(np.array(predictions),axis=0)) --> Since our test target data are elements of {-1,1} and we want to \n",
    "            # have our prediction in the same format, we use the sign function. Hence each column in the accuracy array is like\n",
    "            # [-0.998,1.002,1.24,...,-0.89] and each element represents the combined and weighted prediction of all models up this column\n",
    "            # (so if we are for instance in the 5th column and for the 4th instnace we find the value -0.989, this value represents the \n",
    "            # weighted prediction of a boosted model with 5 base learners for the 4th instance. The 4th instance of the 6th column represents\n",
    "            # the weighted and combined predictions of a boosted model with 6 base learners while the 4th instance of the 4th column represents\n",
    "            # the predction of a model with 4 base learners and so on and so forth...). To make a long story short, we are interested in the \n",
    "            # the sign of these comined predictions. If the sign is positive, we know that the true prediction is more likely postive (1) then\n",
    "            # negaive (-1). The higher the value (postive or negative) the more likely it is that the model returns the correct prediction.\n",
    "            # 4. np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0]) --> With the last step we have transformed the array \n",
    "            # into the shape 8124x1 where the instances are elements {-1,1} and therewith we are now in the situation to compare this \n",
    "            # prediction with our target feature values. The target feature array is of the shape 8124x1 since for each row it contains\n",
    "            # exactly one prediction {-1,1} just as our just created array above --> Ready to compare ;).\n",
    "            # The comparison is done with the == Y_test.values command. As result we get an \n",
    "            # array of the shape 8124x1 where the instances are elements of {True,False} (True if our prediction is consistent with the \n",
    "            # target feature value and False if not). Since we want to calculate a percentage value we have to calculate the fraction of \n",
    "            # instances which have been classified correctly. Therefore we simply sum up the above comparison array \n",
    "            # with the elements {True,False} along the axis 0.\n",
    "            # and divide it by the total number of rows (8124) since True is the same as 1 and False is the same as 0. Hence correct predictions \n",
    "            # increase the sum while false predictions does not change the sum. If we predicted nothing correct the calculation is 0/8124 and \n",
    "            # therewith 0 and if we predicted everything correct, the calculation is 8124/8124 and thereiwth 1. \n",
    "            # 5. self.accuracy.append(np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0])) -->\n",
    "            # After we have computed the above steps, we add the result to the self.accuracy list. This list has the shape n x 1, that is,\n",
    "            # for a model with 5 base learners this list has 5 entries where the 5th entry represents the accuracy of the model when all\n",
    "            # 5 base learners are combined, the 4th element the accuracy of the model when 4 base learners are combined and so on and so forth. This \n",
    "            # procedure has been explained above. That's it and we can plot the accuracy.\n",
    "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n",
    "\n",
    "   \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "######Plot the accuracy of the model against the number of stump-models used##########\n",
    "\n",
    "number_of_base_learners = 50\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "for i in range(number_of_base_learners):\n",
    "    model = Boosting(dataset,i,dataset)\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "\n",
    "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
    "ax0.set_xlabel('# models used for Boosting ')\n",
    "ax0.set_ylabel('accuracy')\n",
    "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1]*100,'%')    \n",
    "                 \n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predictive power of these boosted decision stumps is really amazing. For more than 8000 testing instances we got all correct and therewith an accuracy of 100% with a model which contains 400 base learners. Also interesting is the fact, that the accuracy increases rapidly up to  10 base learners and with  70 base learners our model returns an accuracy of close to 100%.\n",
    "Nevertheless, allow me a side node. First, this kind of boosted model is kind of computationally expensive, that is, its predictive power comes with the costs of computational expensiveness and therewith we have to make a compromise between accuracy and computation effort. Taking the above example we need  400 base learners to get an accuracy of 100% but with  70 decision stumps we already get close to 100%. So we have to decide how important the 100% mark is. Second, the above shown model does not claim to be computationally efficient at all. The model should show how a boosted decision stump can be created from scratch without taking care of computational efficiency. Hence, there are for sure ways how the above code can be made more efficient and therewith the model more fast. It is up to you, playing around with the code and check if you can, for instance, implement some vectorized calculations instead of the loops or something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting with sklearn\n",
    "\n",
    "As always, we will now use the prepackaged sklearn AdaBoostClassifier with the parameters set to the same values we used above. The documentation says:\n",
    "\"An AdaBoost [Y. Freund, R. Schapire, A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting, 1995] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\"[1]\n",
    "The parameters we have to adjust are:\n",
    "\n",
    "- base_estimator: We set this to the default value which is DecisionTreeClassifier as we have used above. Mind that we could also define a variable like estimator= DecisionTreeClassifier and parametrize this estimator by setting max_depth = 1, criterion = \"entropy\",... But for convenience we will omit this here\n",
    "- n_estimators: This is the number of base learners which should be used. We set this to 400 as above.\n",
    "- learning_rate: The default value is 1.0 and reduces the contribution of each tree by the learning rate.We set this to the default value since we don't have explicitly incorporated a learning rate.\n",
    "\n",
    "The rest of the parameter is set to the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "for label in dataset.columns:\n",
    "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
    "    \n",
    "X = dataset.drop(['target'],axis=1)\n",
    "Y = dataset['target']\n",
    "\n",
    "#model = DecisionTreeClassifier(criterion='entropy',max_depth=1)\n",
    "#AdaBoost = AdaBoostClassifier(base_estimator= model,n_estimators=400,learning_rate=1)\n",
    "\n",
    "AdaBoost = AdaBoostClassifier(n_estimators=400,learning_rate=1,algorithm='SAMME')\n",
    "\n",
    "AdaBoost.fit(X,Y)\n",
    "\n",
    "prediction = AdaBoost.score(X,Y)\n",
    "\n",
    "print('The accuracy is: ',prediction*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>e</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>e</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>k</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>o</td>\n",
       "      <td>c</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5  6  7  8  9   ... 13 14 15 16 17 18 19 20 21 22\n",
       "0     p  x  s  n  t  p  f  c  n  k  ...  s  w  w  p  w  o  p  k  s  u\n",
       "1     e  x  s  y  t  a  f  c  b  k  ...  s  w  w  p  w  o  p  n  n  g\n",
       "2     e  b  s  w  t  l  f  c  b  n  ...  s  w  w  p  w  o  p  n  n  m\n",
       "3     p  x  y  w  t  p  f  c  n  n  ...  s  w  w  p  w  o  p  k  s  u\n",
       "4     e  x  s  g  f  n  f  w  b  k  ...  s  w  w  p  w  o  e  n  a  g\n",
       "...  .. .. .. .. .. .. .. .. .. ..  ... .. .. .. .. .. .. .. .. .. ..\n",
       "8119  e  k  s  n  f  n  a  c  b  y  ...  s  o  o  p  o  o  p  b  c  l\n",
       "8120  e  x  s  n  f  n  a  c  b  y  ...  s  o  o  p  n  o  p  b  v  l\n",
       "8121  e  f  s  n  f  n  a  c  b  n  ...  s  o  o  p  o  o  p  b  c  l\n",
       "8122  p  k  y  n  f  y  f  c  n  b  ...  k  w  w  p  w  o  e  w  v  l\n",
       "8123  e  x  s  n  f  n  a  c  b  y  ...  s  o  o  p  o  o  p  o  c  l\n",
       "\n",
       "[8124 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5043</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8124 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  cap-shape  cap-surface  cap-color  bruises  odor  \\\n",
       "5043       1          5            3          9        0     2   \n",
       "3122       1          5            0          3        0     2   \n",
       "2953       0          2            0          4        1     5   \n",
       "6969       1          3            3          2        0     2   \n",
       "6586       1          3            3          4        0     8   \n",
       "...      ...        ...          ...        ...      ...   ...   \n",
       "3230       0          2            3          3        1     5   \n",
       "4194       1          2            2          0        1     2   \n",
       "2107       0          5            3          2        1     5   \n",
       "3976       1          2            0          3        0     2   \n",
       "923        0          5            2          9        1     3   \n",
       "\n",
       "      gill-attachment  gill-spacing  gill-size  gill-color  ...  \\\n",
       "5043                1             0          0           2  ...   \n",
       "3122                1             0          0           7  ...   \n",
       "2953                1             0          0           5  ...   \n",
       "6969                1             0          1           0  ...   \n",
       "6586                1             0          1           0  ...   \n",
       "...               ...           ...        ...         ...  ...   \n",
       "3230                1             0          0           5  ...   \n",
       "4194                1             0          0          10  ...   \n",
       "2107                1             0          0           9  ...   \n",
       "3976                1             0          0           7  ...   \n",
       "923                 1             1          1          10  ...   \n",
       "\n",
       "      stalk-surface-below-ring  stalk-color-above-ring  \\\n",
       "5043                         1                       0   \n",
       "3122                         1                       4   \n",
       "2953                         2                       3   \n",
       "6969                         1                       6   \n",
       "6586                         1                       7   \n",
       "...                        ...                     ...   \n",
       "3230                         2                       7   \n",
       "4194                         0                       7   \n",
       "2107                         2                       6   \n",
       "3976                         1                       4   \n",
       "923                          2                       7   \n",
       "\n",
       "      stalk-color-below-ring  veil-type  veil-color  ring-number  ring-type  \\\n",
       "5043                       6          0           2            1          2   \n",
       "3122                       0          0           2            1          2   \n",
       "2953                       6          0           2            1          4   \n",
       "6969                       7          0           2            1          0   \n",
       "6586                       6          0           2            1          0   \n",
       "...                      ...        ...         ...          ...        ...   \n",
       "3230                       3          0           2            1          4   \n",
       "4194                       7          0           2            1          4   \n",
       "2107                       6          0           2            1          4   \n",
       "3976                       4          0           2            1          2   \n",
       "923                        7          0           2            1          4   \n",
       "\n",
       "      spore-print-color  population  habitat  \n",
       "5043                  1           4        4  \n",
       "3122                  1           5        0  \n",
       "2953                  3           5        0  \n",
       "6969                  7           4        0  \n",
       "6586                  7           4        4  \n",
       "...                 ...         ...      ...  \n",
       "3230                  3           4        0  \n",
       "4194                  1           3        5  \n",
       "2107                  2           5        0  \n",
       "3976                  1           4        1  \n",
       "923                   6           4        0  \n",
       "\n",
       "[8124 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

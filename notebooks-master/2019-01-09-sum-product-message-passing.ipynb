{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Belief Propagation in Python\n",
    "\n",
    "As part of reviewing the ML concepts I learned last year, I implemented the _sum-product message passing_, or [belief propagation](https://en.wikipedia.org/wiki/Belief_propagation), that we learned in our probabilistic modeling course.\n",
    "\n",
    "Belief propagation (or sum-product message passing) is a method that can do inference on [probabilistic graphical models](https://en.wikipedia.org/wiki/Graphical_model). I'll focus on the algorithm that can perform exact inference on tree-like factor graphs.\n",
    "\n",
    "This post assumes knowledge of probabilistic graphical models (perhaps through [the Coursera course](https://www.coursera.org/learn/probabilistic-graphical-models)) and maybe have heard of belief propagation. I'll freely use terms such as \"factor graph\" and \"exact inference.\"\n",
    "\n",
    "\n",
    "## Belief Propagation\n",
    "\n",
    "Belief propagation, or sum-product message passing, is an algorithm for efficiently applying the sum rules and product rules of probability to compute different distributions. For example, if a discrete probability distribution $p(h_1, v_1, h_2, v_2)$ can be factorized as\n",
    "\n",
    "$$p(h_1, h_2, v_1, v_2) = p(h_1)p(h_2 \\mid h_1)p(v_1 \\mid h_1)p(v_2 \\mid h_2),$$\n",
    "\n",
    "I could compute marginals, for example, $p(v_1)$, by multiplying the terms and summing over the other variables.\n",
    "\n",
    "$$p(v_1) =  \\sum_{h_1, h_2, v_2} p(h_1)p(h_2 \\mid h_1)p(v_1 \\mid h_1)p(v_2 \\mid h_2),$$\n",
    "\n",
    "With marginals, one can compute distributions such as $p(v_1)$ and $p(v_1, v_2)$, which means that one can also compute terms like $p(v_2 \\mid v_1)$. Belief propagation provides an efficient method for computing these marginals. \n",
    "\n",
    "This version will only work on discrete distributions. I'll code it with directed graphical models in mind, though it should also work with undirected models with few changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: (Digression) Representing probability distributions as numpy arrays\n",
    "\n",
    "The sum-product message passing involves representing, summing, and multiplying discrete distributions. I think it's pretty fun to try to implement this with numpy arrays; I gained more intuition about probability distributions and numpy.\n",
    "\n",
    "A discrete conditional distribution $p(v_1 \\mid h_1)$ can be represented as an array with two axes, such as\n",
    "\n",
    "| | $h_1$ = a | $h_1$ = b | $h_1$ = c |\n",
    "|-|-|-|-|\n",
    "| $v_1$ = 0 | 0.4 | 0.8 | 0.9 |\n",
    "| $v_1$ = 1 | 0.6 | 0.2 | 0.1 |\n",
    "\n",
    "\n",
    "Using an axis for each variable can generalize to more variables. For example, the 5-variable $p(h_5 \\mid h_4, h_3, h_2, h_1)$ could be represented by an array with five axes. \n",
    "\n",
    "It's useful to label axes with variable names. I'll do this in my favorite way, a little `namedtuple`! (It's kind of like a janky version of the [NamedTensor](http://nlp.seas.harvard.edu/NamedTensor).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledArray = namedtuple('LabeledArray', [\n",
    "    'array',\n",
    "    'axes_labels',\n",
    "])\n",
    "\n",
    "def name_to_axis_mapping(labeled_array):\n",
    "    return {\n",
    "        name: axis\n",
    "        for axis, name in enumerate(labeled_array.axes_labels)\n",
    "    }\n",
    "\n",
    "def other_axes_from_labeled_axes(labeled_array, axis_label):\n",
    "    # returns the indexes of the axes that are not axis label\n",
    "    return tuple(\n",
    "        axis\n",
    "        for axis, name in enumerate(labeled_array.axes_labels)\n",
    "        if name != axis_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that a numpy array is a valid discrete distribution\n",
    "\n",
    "It's easy to accidentally swap axes when creating numpy arrays representing distributions. I'll also write code to verify they are valid distributions.\n",
    "\n",
    "To check that a multidimensional array is a _joint_ distribution, the entire array should sum to one.\n",
    "\n",
    "To check that a 2D array is a _conditional_ distribution, when all of the right-hand-side variables have been assigned, such as $p(v_1 \\mid h_1 = a)$, the resulting vector represents a distribution. The vector should have the length of the number of states of $v_1$ and should sum to one. Computing this in numpy involves summing along the axis corresponding to the $v_1$ variable.\n",
    "\n",
    "To generalize conditional distribution arrays to the multi-dimensional example, again, when all of the right-hand-side variables have been assigned, such as $p(h_5 \\mid h_4=a, h_3=b, h_2=a, h_1=a)$, the resulting vector represents a distribution. The vector should have a length which is the number of states of $h_1$ and should sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_conditional_prob(labeled_array, var_name):\n",
    "    '''\n",
    "    labeled_array (LabeledArray)\n",
    "    variable (str): name of variable, i.e. 'a' in p(a|b)\n",
    "    '''\n",
    "    return np.all(np.isclose(np.sum(\n",
    "        labeled_array.array,\n",
    "        axis=name_to_axis_mapping(labeled_array)[var_name]\n",
    "    ), 1.0))\n",
    "    \n",
    "def is_joint_prob(labeled_array):\n",
    "    return np.all(np.isclose(np.sum(labeled_array.array), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_v1_given_h1 = LabeledArray(np.array([[0.4, 0.8, 0.9], [0.6, 0.2, 0.1]]), ['v1', 'h1'])\n",
    "\n",
    "p_h1 = LabeledArray(np.array([0.6, 0.3, 0.1]), ['h1'])\n",
    "\n",
    "p_v1_given_many = LabeledArray(np.array(\n",
    "    [[[0.9, 0.2], [0.3, 0.2]],\n",
    "     [[0.1, 0.8], [0.7, 0.8]]]\n",
    "), ['v1', 'h1', 'h2'])\n",
    "\n",
    "assert is_conditional_prob(p_v1_given_h1, 'v1')\n",
    "assert not is_joint_prob(p_v1_given_h1)\n",
    "\n",
    "assert is_conditional_prob(p_h1, 'h1')\n",
    "assert is_joint_prob(p_h1)\n",
    "\n",
    "assert is_conditional_prob(p_v1_given_many, 'v1')\n",
    "assert not is_joint_prob(p_v1_given_many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying distributions\n",
    "\n",
    "In belief propagation, I also need to compute the product of distributions, such as $p(h_2 \\mid h_1)p(h_1)$.\n",
    "\n",
    "In this case, I'll only need to multiply a multidimensional array by a 1D array and occasionally a scalar. The way I ended up implementing this was to align the axis of the 1D array with its corresponding axis from the other distribution. Then I tile the 1D array to be the size of $p(h_2 \\mid h_1)$. This gives me the joint distribution $p(h_1, h_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_to_shape_along_axis(arr, target_shape, target_axis):\n",
    "    # get a list of all axes\n",
    "    raw_axes = list(range(len(target_shape)))\n",
    "    tile_dimensions = [target_shape[a] for a in raw_axes if a != target_axis]\n",
    "    if len(arr.shape) == 0:\n",
    "        # If given a scalar, also tile it in the target dimension (so it's a bunch of 1s)\n",
    "        tile_dimensions += [target_shape[target_axis]]\n",
    "    elif len(arr.shape) == 1:\n",
    "        # If given an array, it should be the same shape as the target axis\n",
    "        assert arr.shape[0] == target_shape[target_axis]\n",
    "        tile_dimensions += [1]\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    tiled = np.tile(arr, tile_dimensions)\n",
    "\n",
    "    # Tiling only adds prefix axes, so rotate this one back into place\n",
    "    shifted_axes = raw_axes[:target_axis] + [raw_axes[-1]] + raw_axes[target_axis:-1]\n",
    "    transposed = np.transpose(tiled, shifted_axes)\n",
    "\n",
    "    # Double-check this code tiled it to the correct shape\n",
    "    assert transposed.shape == target_shape\n",
    "    return transposed\n",
    "\n",
    "def tile_to_other_dist_along_axis_name(tiling_labeled_array, target_array):\n",
    "    assert len(tiling_labeled_array.axes_labels) == 1\n",
    "    target_axis_label = tiling_labeled_array.axes_labels[0]\n",
    "    \n",
    "    return LabeledArray(\n",
    "        tile_to_shape_along_axis(\n",
    "            tiling_labeled_array.array,\n",
    "            target_array.array.shape,\n",
    "            name_to_axis_mapping(target_array)[target_axis_label]\n",
    "        ),\n",
    "        axes_labels=target_array.axes_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_p_h1 = tile_to_other_dist_along_axis_name(p_h1, p_v1_given_h1)\n",
    "\n",
    "# Check that the product is a joint distribution (p(v1, h1))\n",
    "assert np.isclose(np.sum(p_v1_given_h1.array * tiled_p_h1.array), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Factor Graphs\n",
    "\n",
    "Factor graphs are used to represent a distribution for sum-product message passing.\n",
    "One factor graph that represents $p(h_1, h_2, v_1, v_2)$ is\n",
    "\n",
    "![](images/2019-01-09-factor-graph.png)\n",
    "\n",
    "Factors, such as $p(h_1)$, are represented by black squares and represent a factor (or function, such as a probability distribution.) Variables, such as $h_1$, are represented by white circles. Variables only neighbor factors, and factors only neighbor variables.\n",
    "\n",
    "In code, \n",
    " - There are two classes in the graph: Variable and Factor. Both classes have a string representing the name and a list of neighbors.\n",
    " - A Variable can only have Factors in its list of neighbors. A Factor can only have Variables.\n",
    " - To represent the probability distribution, Factors also have a field for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.neighbors = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{classname}({name}, [{neighbors}])\".format(\n",
    "            classname=type(self).__name__,\n",
    "            name=self.name,\n",
    "            neighbors=', '.join([n.name for n in self.neighbors])\n",
    "        )\n",
    "\n",
    "    def is_valid_neighbor(self, neighbor):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def add_neighbor(self, neighbor):\n",
    "        assert self.is_valid_neighbor(neighbor)\n",
    "        self.neighbors.append(neighbor)\n",
    "\n",
    "\n",
    "class Variable(Node):\n",
    "    def is_valid_neighbor(self, factor):\n",
    "        return isinstance(factor, Factor)  # Variables can only neighbor Factors\n",
    "\n",
    "\n",
    "class Factor(Node):\n",
    "    def is_valid_neighbor(self, variable):\n",
    "        return isinstance(variable, Variable)  # Factors can only neighbor Variables\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super(Factor, self).__init__(name)\n",
    "        self.data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing distributions into graphs\n",
    "\n",
    "Defining a graph can be a little verbose. I can hack together a parser for probability distributions that can interpret a string like `p(h1)p(h2∣h1)p(v1∣h1)p(v2∣h2)` as a factor graph for me.\n",
    "\n",
    "(This is pretty fragile and not user-friendly. For example, be sure to use `|` character rather than the indistinguishable `∣` character!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParsedTerm = namedtuple('ParsedTerm', [\n",
    "    'term',\n",
    "    'var_name',\n",
    "    'given',\n",
    "])\n",
    "\n",
    "\n",
    "def _parse_term(term):\n",
    "    # Given a term like (a|b,c), returns a list of variables\n",
    "    # and conditioned-on variables\n",
    "    assert term[0] == '(' and term[-1] == ')'\n",
    "    term_variables = term[1:-1]\n",
    "\n",
    "    # Handle conditionals\n",
    "    if '|' in term_variables:\n",
    "        var, given = term_variables.split('|')\n",
    "        given = given.split(',')\n",
    "    else:\n",
    "        var = term_variables\n",
    "        given = []\n",
    "\n",
    "    return var, given\n",
    "\n",
    "\n",
    "def _parse_model_string_into_terms(model_string):\n",
    "    return [\n",
    "        ParsedTerm('p' + term, *_parse_term(term))\n",
    "        for term in model_string.split('p')\n",
    "        if term\n",
    "    ]\n",
    "\n",
    "def parse_model_into_variables_and_factors(model_string):\n",
    "    # Takes in a model_string such as p(h1)p(h2∣h1)p(v1∣h1)p(v2∣h2) and returns a\n",
    "    # dictionary of variable names to variables and a list of factors.\n",
    "    \n",
    "    # Split model_string into ParsedTerms\n",
    "    parsed_terms = _parse_model_string_into_terms(model_string)\n",
    "    \n",
    "    # First, extract all of the variables from the model_string (h1, h2, v1, v2). \n",
    "    # These each will be a new Variable that are referenced from Factors below.\n",
    "    variables = {}\n",
    "    for parsed_term in parsed_terms:\n",
    "        # if the variable name wasn't seen yet, add it to the variables dict\n",
    "        if parsed_term.var_name not in variables:\n",
    "            variables[parsed_term.var_name] = Variable(parsed_term.var_name)\n",
    "\n",
    "    # Now extract factors from the model. Each term (e.g. \"p(v1|h1)\") corresponds to \n",
    "    # a factor. \n",
    "    # Then find all variables in this term (\"v1\", \"h1\") and add the corresponding Variables\n",
    "    # as neighbors to the new Factor, and this Factor to the Variables' neighbors.\n",
    "    factors = []\n",
    "    for parsed_term in parsed_terms:\n",
    "        # This factor will be neighbors with all \"variables\" (left-hand side variables) and given variables\n",
    "        new_factor = Factor(parsed_term.term)\n",
    "        all_var_names = [parsed_term.var_name] + parsed_term.given\n",
    "        for var_name in all_var_names:\n",
    "            new_factor.add_neighbor(variables[var_name])\n",
    "            variables[var_name].add_neighbor(new_factor)\n",
    "        factors.append(new_factor)\n",
    "\n",
    "    return factors, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Factor(p(h1), [h1]),\n",
       "  Factor(p(h2|h1), [h2, h1]),\n",
       "  Factor(p(v1|h1), [v1, h1]),\n",
       "  Factor(p(v2|h2), [v2, h2])],\n",
       " {'h1': Variable(h1, [p(h1), p(h2|h1), p(v1|h1)]),\n",
       "  'h2': Variable(h2, [p(h2|h1), p(v2|h2)]),\n",
       "  'v1': Variable(v1, [p(v1|h1)]),\n",
       "  'v2': Variable(v2, [p(v2|h2)])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_model_into_variables_and_factors(\"p(h1)p(h2|h1)p(v1|h1)p(v2|h2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ([Factor(p(h1), [h1]),\n",
    "      Factor(p(h2|h1), [h2, h1]),\n",
    "      Factor(p(v1|h1), [v1, h1]),\n",
    "      Factor(p(v2|h2), [v2, h2])],\n",
    "     {'h1': Variable(h1, [p(h1), p(h2|h1), p(v1|h1)]),\n",
    "      'h2': Variable(h2, [p(h2|h1), p(v2|h2)]),\n",
    "      'v1': Variable(v1, [p(v1|h1)]),\n",
    "      'v2': Variable(v2, [p(v2|h2)])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adding distributions to the graph\n",
    "\n",
    "Before I can run the algorithm, I need to associate LabeledArrays with each Factor. At this point, I'll create a class to hold onto the Variables and Factors.\n",
    "\n",
    "While I'm here, I can do a few checks to make sure the provided data matches the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGM(object):\n",
    "    def __init__(self, factors, variables):\n",
    "        self._factors = factors\n",
    "        self._variables = variables\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, model_string):\n",
    "        factors, variables = parse_model_into_variables_and_factors(model_string)\n",
    "        return PGM(factors, variables)\n",
    "\n",
    "    def set_data(self, data):\n",
    "        # Keep track of variable dimensions to check for shape mistakes\n",
    "        var_dims = {}\n",
    "        for factor in self._factors:\n",
    "            factor_data = data[factor.name]\n",
    "\n",
    "            if set(factor_data.axes_labels) != set(v.name for v in factor.neighbors):\n",
    "                missing_axes = set(v.name for v in factor.neighbors) - set(data[factor.name].axes_labels)\n",
    "                raise ValueError(\"data[{}] is missing axes: {}\".format(factor.name, missing_axes))\n",
    "                \n",
    "            for var_name, dim in zip(factor_data.axes_labels, factor_data.array.shape):\n",
    "                if var_name not in var_dims:\n",
    "                    var_dims[var_name] = dim\n",
    "    \n",
    "                if var_dims[var_name] != dim:\n",
    "                    raise ValueError(\"data[{}] axes is wrong size, {}. Expected {}\".format(factor.name, dim, var_dims[var_name]))            \n",
    "                    \n",
    "            factor.data = data[factor.name]\n",
    "            \n",
    "    def variable_from_name(self, var_name):\n",
    "        return self._variables[var_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now try to add distributions to a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_h1 = LabeledArray(np.array([[0.2], [0.8]]), ['h1'])\n",
    "p_h2_given_h1 = LabeledArray(np.array([[0.5, 0.2], [0.5, 0.8]]), ['h2', 'h1'])\n",
    "p_v1_given_h1 = LabeledArray(np.array([[0.6, 0.1], [0.4, 0.9]]), ['v1', 'h1'])\n",
    "p_v2_given_h2 = LabeledArray(p_v1_given_h1.array, ['v2', 'h2'])\n",
    "\n",
    "assert is_joint_prob(p_h1)\n",
    "assert is_conditional_prob(p_h2_given_h1, 'h2')\n",
    "assert is_conditional_prob(p_v1_given_h1, 'v1')\n",
    "assert is_conditional_prob(p_v2_given_h2, 'v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = PGM.from_string(\"p(h1)p(h2|h1)p(v1|h1)p(v2|h2)\")\n",
    "\n",
    "pgm.set_data({\n",
    "    \"p(h1)\": p_h1,\n",
    "    \"p(h2|h1)\": p_h2_given_h1,\n",
    "    \"p(v1|h1)\": p_v1_given_h1,\n",
    "    \"p(v2|h2)\": p_v2_given_h2,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Belief Propagation\n",
    "\n",
    "We made it! Now we can implement sum-product message passing. \n",
    "\n",
    "Sum-product message passing will compute values (\"messages\") for every edge in the factor graph.\n",
    "\n",
    "![](images/2019-01-09-factor-graph.png)\n",
    "\n",
    "The algorithm will compute a message from the Factor $f$ to the Variable $x$, notated as $\\mu_{f \\to x}(x)$. It will also compute the value from Variable $x$ to the Factor $f$, $\\mu_{x \\to f}(x)$. As is common in graph algorithms, these are defined recursively.\n",
    "\n",
    "(I'm using the equations as given in Barber p84.)\n",
    "\n",
    "### Variable-to-Factor Message\n",
    "\n",
    "The variable-to-factor message is given by:\n",
    "\n",
    "$$\\mu_{x \\to f}(x) = \\prod_{g \\in \\{ne(x) \\setminus f\\}} \\mu_{g \\to x}(x)$$\n",
    "\n",
    "where $ne(x)$ are the neighbors of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _variable_to_factor_messages(variable, factor):\n",
    "    # Take the product over all incoming factors into this variable except the variable\n",
    "    incoming_messages = [\n",
    "        _factor_to_variable_message(neighbor_factor, variable)\n",
    "        for neighbor_factor in variable.neighbors\n",
    "        if neighbor_factor.name != factor.name\n",
    "    ]\n",
    "\n",
    "    # If there are no incoming messages, this is 1\n",
    "    return np.prod(incoming_messages, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor-to-Variable Message\n",
    "\n",
    "The variable-to-factor message is given by \n",
    "\n",
    "$$\\mu_{f \\to x}(x) = \\sum_{\\chi_f \\setminus x}\\phi_f(\\chi_f) \\prod_{y \\in \\{ne(f) \\setminus x \\}} \\mu_{y \\to f}(y)$$\n",
    "\n",
    "In the case of probabilities, $\\phi_f(\\chi_f)$ is the probability distribution associated with the factor, and $\\sum_{\\chi_f \\setminus x}$ sums over all variables except $x$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _factor_to_variable_messages(factor, variable):\n",
    "    # Compute the product\n",
    "    factor_dist = np.copy(factor.data.array)\n",
    "    for neighbor_variable in factor.neighbors:\n",
    "        if neighbor_variable.name == variable.name:\n",
    "            continue\n",
    "        incoming_message = variable_to_factor_messages(neighbor_variable, factor)\n",
    "        factor_dist *= tile_to_other_dist_along_axis_name(\n",
    "            LabeledArray(incoming_message, [neighbor_variable.name]),\n",
    "            factor.data\n",
    "        ).array\n",
    "    # Sum over the axes that aren't `variable`\n",
    "    other_axes = other_axes_from_labeled_axes(factor.data, variable.name)\n",
    "    return np.squeeze(np.sum(factor_dist, axis=other_axes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal\n",
    "\n",
    "The marginal of a variable $x$ is given by\n",
    "\n",
    "$$p(x) \\propto \\prod_{f \\in ne(x)}\\mu_{f \\to x}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal(variable):\n",
    "    # p(variable) is proportional to the product of incoming messages to variable.\n",
    "    unnorm_p = np.prod([\n",
    "        self.factor_to_variable_message(neighbor_factor, variable)\n",
    "        for neighbor_factor in variable.neighbors\n",
    "    ], axis=0)\n",
    "\n",
    "    # At this point, we can normalize this distribution\n",
    "    return unnorm_p/np.sum(unnorm_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to PGM\n",
    "\n",
    "A source of message passing's efficiency is that messages from one computation can be reused by other computations. I'll create an object to store `Messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Messages(object):\n",
    "    def __init__(self):\n",
    "        self.messages = {}\n",
    "        \n",
    "    def _variable_to_factor_messages(self, variable, factor):\n",
    "        # Take the product over all incoming factors into this variable except the variable\n",
    "        incoming_messages = [\n",
    "            self.factor_to_variable_message(neighbor_factor, variable)\n",
    "            for neighbor_factor in variable.neighbors\n",
    "            if neighbor_factor.name != factor.name\n",
    "        ]\n",
    "\n",
    "        # If there are no incoming messages, this is 1\n",
    "        return np.prod(incoming_messages, axis=0)\n",
    "    \n",
    "    def _factor_to_variable_messages(self, factor, variable):\n",
    "        # Compute the product\n",
    "        factor_dist = np.copy(factor.data.array)\n",
    "        for neighbor_variable in factor.neighbors:\n",
    "            if neighbor_variable.name == variable.name:\n",
    "                continue\n",
    "            incoming_message = self.variable_to_factor_messages(neighbor_variable, factor)\n",
    "            factor_dist *= tile_to_other_dist_along_axis_name(\n",
    "                LabeledArray(incoming_message, [neighbor_variable.name]),\n",
    "                factor.data\n",
    "            ).array\n",
    "        # Sum over the axes that aren't `variable`\n",
    "        other_axes = other_axes_from_labeled_axes(factor.data, variable.name)\n",
    "        return np.squeeze(np.sum(factor_dist, axis=other_axes))\n",
    "    \n",
    "    def marginal(self, variable):\n",
    "        # p(variable) is proportional to the product of incoming messages to variable.\n",
    "        unnorm_p = np.prod([\n",
    "            self.factor_to_variable_message(neighbor_factor, variable)\n",
    "            for neighbor_factor in variable.neighbors\n",
    "        ], axis=0)\n",
    "\n",
    "        # At this point, we can normalize this distribution\n",
    "        return unnorm_p/np.sum(unnorm_p)\n",
    "    \n",
    "    def variable_to_factor_messages(self, variable, factor):\n",
    "        message_name = (variable.name, factor.name)\n",
    "        if message_name not in self.messages:\n",
    "            self.messages[message_name] = self._variable_to_factor_messages(variable, factor)\n",
    "        return self.messages[message_name]\n",
    "        \n",
    "    def factor_to_variable_message(self, factor, variable):\n",
    "        message_name = (factor.name, variable.name)\n",
    "        if message_name not in self.messages:\n",
    "            self.messages[message_name] = self._factor_to_variable_messages(factor, variable)\n",
    "        return self.messages[message_name]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23, 0.77])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgm = PGM.from_string(\"p(h1)p(h2|h1)p(v1|h1)p(v2|h2)\")\n",
    "\n",
    "pgm.set_data({\n",
    "    \"p(h1)\": p_h1,\n",
    "    \"p(h2|h1)\": p_h2_given_h1,\n",
    "    \"p(v1|h1)\": p_v1_given_h1,\n",
    "    \"p(v2|h2)\": p_v2_given_h2,\n",
    "})\n",
    "\n",
    "m = Messages()\n",
    "m.marginal(pgm.variable_from_name('v2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    array([0.23, 0.77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('p(h1)', 'h1'): array([0.2, 0.8]),\n",
       " ('v1', 'p(v1|h1)'): 1.0,\n",
       " ('p(v1|h1)', 'h1'): array([1., 1.]),\n",
       " ('h1', 'p(h2|h1)'): array([0.2, 0.8]),\n",
       " ('p(h2|h1)', 'h2'): array([0.26, 0.74]),\n",
       " ('h2', 'p(v2|h2)'): array([0.26, 0.74]),\n",
       " ('p(v2|h2)', 'v2'): array([0.23, 0.77])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {('p(h1)', 'h1'): array([0.2, 0.8]),\n",
    "     ('v1', 'p(v1|h1)'): 1.0,\n",
    "     ('p(v1|h1)', 'h1'): array([1., 1.]),\n",
    "     ('h1', 'p(h2|h1)'): array([0.2, 0.8]),\n",
    "     ('p(h2|h1)', 'h2'): array([0.26, 0.74]),\n",
    "     ('h2', 'p(v2|h2)'): array([0.26, 0.74]),\n",
    "     ('p(v2|h2)', 'v2'): array([0.23, 0.77])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.marginal(pgm.variable_from_name('v1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    array([0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example from book\n",
    "\n",
    "Example 5.1 on p79 of Barber  has a numerical example. I can make sure I get the same values (`[0.5746, 0.318 , 0.1074]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5746, 0.318 , 0.1074])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgm = PGM.from_string(\"p(x5|x4)p(x4|x3)p(x3|x2)p(x2|x1)p(x1)\")\n",
    "\n",
    "p_x5_given_x4 = LabeledArray(np.array([[0.7, 0.5, 0], [0.3, 0.3, 0.5], [0, 0.2, 0.5]]), ['x5', 'x4'])\n",
    "assert is_conditional_prob(p_x5_given_x4, 'x5')\n",
    "p_x4_given_x3 = LabeledArray(p_x5_given_x4.array, ['x4', 'x3'])\n",
    "p_x3_given_x2 = LabeledArray(p_x5_given_x4.array, ['x3', 'x2'])\n",
    "p_x2_given_x1 = LabeledArray(p_x5_given_x4.array, ['x2', 'x1'])\n",
    "p_x1 = LabeledArray(np.array([1, 0, 0]), ['x1'])\n",
    "\n",
    "pgm.set_data({\n",
    "    \"p(x5|x4)\": p_x5_given_x4,\n",
    "    \"p(x4|x3)\": p_x4_given_x3,\n",
    "    \"p(x3|x2)\": p_x3_given_x2,\n",
    "    \"p(x2|x1)\": p_x2_given_x1,\n",
    "    \"p(x1)\": p_x1,\n",
    "})\n",
    "\n",
    "Messages().marginal(pgm.variable_from_name('x5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Also\n",
    "\n",
    " - In my previous post [HMM](2018-05-02-hmm-alpha-recursion.ipynb), I implemented a form of belief propagation for Hidden Markov Models called Alpha Recursion.\n",
    " - Python library [pgmpy](https://github.com/pgmpy/pgmpy) does probabilistic graphical models and has nice code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
